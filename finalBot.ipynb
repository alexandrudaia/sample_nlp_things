{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27775fc",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25c86f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-telegram-bot openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7793601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No error handlers are registered, logging exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_application.py\", line 1325, in process_update\n",
      "    await coroutine\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_handlers\\basehandler.py\", line 157, in handle_update\n",
      "    return await self.callback(update, context)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Temp\\ipykernel_11796\\3592400371.py\", line 83, in handle_message\n",
      "    answer = get_answer_from_openai(question, context_text)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Temp\\ipykernel_11796\\3592400371.py\", line 37, in get_answer_from_openai\n",
      "    response = openai.Completion.create(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 226, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 619, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 682, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?\n",
      "No error handlers are registered, logging exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_application.py\", line 1325, in process_update\n",
      "    await coroutine\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_handlers\\basehandler.py\", line 157, in handle_update\n",
      "    return await self.callback(update, context)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Temp\\ipykernel_11796\\3592400371.py\", line 83, in handle_message\n",
      "    answer = get_answer_from_openai(question, context_text)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Temp\\ipykernel_11796\\3592400371.py\", line 37, in get_answer_from_openai\n",
      "    response = openai.Completion.create(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 226, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 619, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 682, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?\n",
      "No error handlers are registered, logging exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_application.py\", line 1325, in process_update\n",
      "    await coroutine\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_handlers\\basehandler.py\", line 157, in handle_update\n",
      "    return await self.callback(update, context)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Temp\\ipykernel_11796\\3592400371.py\", line 83, in handle_message\n",
      "    answer = get_answer_from_openai(question, context_text)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Temp\\ipykernel_11796\\3592400371.py\", line 37, in get_answer_from_openai\n",
      "    response = openai.Completion.create(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 226, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 619, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 682, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?\n",
      "No error handlers are registered, logging exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_application.py\", line 1325, in process_update\n",
      "    await coroutine\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_handlers\\basehandler.py\", line 157, in handle_update\n",
      "    return await self.callback(update, context)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Temp\\ipykernel_11796\\3592400371.py\", line 83, in handle_message\n",
      "    answer = get_answer_from_openai(question, context_text)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Temp\\ipykernel_11796\\3592400371.py\", line 37, in get_answer_from_openai\n",
      "    response = openai.Completion.create(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 226, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 619, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 682, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot close a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m     application\u001b[38;5;241m.\u001b[39mrun_polling()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 102\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m application\u001b[38;5;241m.\u001b[39madd_handler(MessageHandler(filters\u001b[38;5;241m.\u001b[39mTEXT \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39mfilters\u001b[38;5;241m.\u001b[39mCOMMAND, handle_message))\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Start the bot\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m \u001b[43mapplication\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_polling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_application.py:871\u001b[0m, in \u001b[0;36mApplication.run_polling\u001b[1;34m(self, poll_interval, timeout, bootstrap_retries, read_timeout, write_timeout, connect_timeout, pool_timeout, allowed_updates, drop_pending_updates, close_loop, stop_signals)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_callback\u001b[39m(exc: TelegramError) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_error(error\u001b[38;5;241m=\u001b[39mexc, update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m--> 871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdater_coroutine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdater\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_polling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoll_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbootstrap_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbootstrap_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnect_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnect_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallowed_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_updates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_pending_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_pending_updates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# if there is an error in fetching updates\u001b[39;49;00m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclose_loop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclose_loop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_signals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_application.py:1099\u001b[0m, in \u001b[0;36mApplication.__run\u001b[1;34m(self, updater_coroutine, stop_signals, close_loop)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m close_loop:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\selector_events.py:84\u001b[0m, in \u001b[0;36mBaseSelectorEventLoop.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot close a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_closed():\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot close a running event loop"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import nest_asyncio\n",
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key='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'\n",
    "\n",
    "def read_file(file_path, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Reads the content of a text file and returns it as a string.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the text file.\n",
    "        encoding (str): The encoding of the text file.\n",
    "    \n",
    "    Returns:\n",
    "        str: Content of the text file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        return file.read()\n",
    "\n",
    "def get_answer_from_openai(question, context):\n",
    "    \"\"\"\n",
    "    Uses OpenAI API to get an answer for the question based on the provided context.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask.\n",
    "        context (str): The context to provide to the model.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated answer from OpenAI.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        #engine='gpt-3.5-turbo-instruct',\n",
    "        engine='gpt-4',\n",
    "        prompt=f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\",\n",
    "        max_tokens=75,\n",
    "        temperature=0.13,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=['\\n']\n",
    "    )\n",
    "    answer = response.choices[0].text.strip()\n",
    "    return answer\n",
    "\n",
    "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    \"\"\"\n",
    "    Sends a welcome message when the /start command is issued.\n",
    "    \n",
    "    Args:\n",
    "        update (Update): The update object that contains the message.\n",
    "        context (ContextTypes.DEFAULT_TYPE): The context object that contains the bot's data.\n",
    "    \"\"\"\n",
    "    await update.message.reply_text('Hi! Send me a question and I will answer it based on the provided context.')\n",
    "\n",
    "async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    \"\"\"\n",
    "    Handles incoming messages and provides answers using the OpenAI API.\n",
    "    \n",
    "    Args:\n",
    "        update (Update): The update object that contains the message.\n",
    "        context (ContextTypes.DEFAULT_TYPE): The context object that contains the bot's data.\n",
    "    \"\"\"\n",
    "    question = update.message.text\n",
    "    \n",
    "    # Paths to your text files\n",
    "    file1_path = 'page_corpus.txt'\n",
    " \n",
    "    \n",
    "    # Read the content of the text files\n",
    "    file1_content = read_file(file1_path)\n",
    "    #file2_content = read_file(file2_path)\n",
    "    \n",
    "    # Combine the contents into a single context\n",
    "    context_text = file1_content\n",
    "    \n",
    "    # Get the answer from OpenAI\n",
    "    answer = get_answer_from_openai(question, context_text)\n",
    "    \n",
    "    # Send the answer back to the user\n",
    "    await update.message.reply_text(answer)\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Starts the Telegram bot and sets up the command and message handlers.\n",
    "    \"\"\"\n",
    "    # Replace 'YOUR_TELEGRAM_BOT_TOKEN' with your actual Telegram bot token\n",
    "    application = ApplicationBuilder().token(\"7846009421:AAHIxcPIEjbwF0t7f9x8M9469lHwZAJsCUI\").build()\n",
    "\n",
    "    # Add command handler for the /start command\n",
    "    application.add_handler(CommandHandler(\"start\", start))\n",
    "    \n",
    "    # Add message handler for text messages\n",
    "    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
    "\n",
    "    # Start the bot\n",
    "    application.run_polling()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68b6de7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot close a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m     application\u001b[38;5;241m.\u001b[39mrun_polling()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 63\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m application\u001b[38;5;241m.\u001b[39madd_handler(MessageHandler(filters\u001b[38;5;241m.\u001b[39mTEXT \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39mfilters\u001b[38;5;241m.\u001b[39mCOMMAND, handle_message))\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Start the bot\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m \u001b[43mapplication\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_polling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_application.py:871\u001b[0m, in \u001b[0;36mApplication.run_polling\u001b[1;34m(self, poll_interval, timeout, bootstrap_retries, read_timeout, write_timeout, connect_timeout, pool_timeout, allowed_updates, drop_pending_updates, close_loop, stop_signals)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_callback\u001b[39m(exc: TelegramError) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_error(error\u001b[38;5;241m=\u001b[39mexc, update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m--> 871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdater_coroutine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdater\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_polling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoll_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbootstrap_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbootstrap_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnect_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnect_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallowed_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_updates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_pending_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_pending_updates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# if there is an error in fetching updates\u001b[39;49;00m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclose_loop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclose_loop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_signals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\telegram\\ext\\_application.py:1099\u001b[0m, in \u001b[0;36mApplication.__run\u001b[1;34m(self, updater_coroutine, stop_signals, close_loop)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m close_loop:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\selector_events.py:84\u001b[0m, in \u001b[0;36mBaseSelectorEventLoop.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot close a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_closed():\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot close a running event loop"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import nest_asyncio\n",
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "def read_file(file_path, encoding='utf-8'):\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        return file.read()\n",
    "\n",
    "def get_answer_from_openai(question, context):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"}\n",
    "        ],\n",
    "        max_tokens=75,\n",
    "        temperature=0.13,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    answer = response.choices[0].message['content'].strip()\n",
    "    return answer\n",
    "\n",
    "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    await update.message.reply_text('Hi! Send me a question and I will answer it based on the provided context.')\n",
    "\n",
    "async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
    "    question = update.message.text\n",
    "    \n",
    "    # Paths to your text files\n",
    "    file1_path = 'page_corpus.txt'\n",
    "    \n",
    "    # Read the content of the text files\n",
    "    file1_content = read_file(file1_path)\n",
    "    \n",
    "    # Combine the contents into a single context\n",
    "    context_text = file1_content\n",
    "    \n",
    "    # Get the answer from OpenAI\n",
    "    answer = get_answer_from_openai(question, context_text)\n",
    "    \n",
    "    # Send the answer back to the user\n",
    "    await update.message.reply_text(answer)\n",
    "\n",
    "def main() -> None:\n",
    "    application = ApplicationBuilder().token(\"7846009421:AAHIxcPIEjbwF0t7f9x8M9469lHwZAJsCUI\").build()\n",
    "\n",
    "    # Add command handler for the /start command\n",
    "    application.add_handler(CommandHandler(\"start\", start))\n",
    "    \n",
    "    # Add message handler for text messages\n",
    "    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
    "\n",
    "    # Start the bot\n",
    "    application.run_polling()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447cea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
