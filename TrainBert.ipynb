{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac780a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "771f3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('new_qa.xlsx')\n",
    "df = df.rename(columns={\"target_text\":\"label\", \"source_text\":\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d61dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q=['Give me the SNP-disease/trait associations of the following paragraph in JSON format (rsID, disease, allele, genotype, odds ratio, p-value, ethnicity, sex,...):']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b787948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Three of the four SNPs were significantly ass...</td>\n",
       "      <td>[ { \"genotype/allele\": \"-278 A/G\", \"rsID\": nul...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None of the variant alleles at any of these t...</td>\n",
       "      <td>[ { \"genotype/allele\": \"C\", \"rsID\": \"rs3781117...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Of the additional SNPs genotyped, rs4899445 d...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs489...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The variant genotypes at both SNP loci were s...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs125...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In SNP rs5498 E469(A/G), similar results were...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"AA\",\\n\"rsID\": \"rs549...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>However, the variant rs1052700*A was associat...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"A\",\\n\"rsID\": \"rs1052...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>The rs401681[C] allele has been positively as...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs4016...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>In the MSA-C subgroup the association strengt...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>We repeated the analysis in the MSA-C subgrou...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Two SNPs were significantly associated with i...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs5277...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              context  \\\n",
       "0    Three of the four SNPs were significantly ass...   \n",
       "1    None of the variant alleles at any of these t...   \n",
       "2    Of the additional SNPs genotyped, rs4899445 d...   \n",
       "3    The variant genotypes at both SNP loci were s...   \n",
       "4    In SNP rs5498 E469(A/G), similar results were...   \n",
       "..                                                ...   \n",
       "95   However, the variant rs1052700*A was associat...   \n",
       "96   The rs401681[C] allele has been positively as...   \n",
       "97   In the MSA-C subgroup the association strengt...   \n",
       "98   We repeated the analysis in the MSA-C subgrou...   \n",
       "99   Two SNPs were significantly associated with i...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   [ { \"genotype/allele\": \"-278 A/G\", \"rsID\": nul...   \n",
       "1   [ { \"genotype/allele\": \"C\", \"rsID\": \"rs3781117...   \n",
       "2   [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs489...   \n",
       "3   [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs125...   \n",
       "4   [\\n{\\n\"genotype/allele\": \"AA\",\\n\"rsID\": \"rs549...   \n",
       "..                                                ...   \n",
       "95  [\\n{\\n\"genotype/allele\": \"A\",\\n\"rsID\": \"rs1052...   \n",
       "96  [\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs4016...   \n",
       "97  [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...   \n",
       "98  [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...   \n",
       "99  [\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs5277...   \n",
       "\n",
       "                                             question  \n",
       "0   Give me the SNP-disease/trait associations of ...  \n",
       "1   Give me the SNP-disease/trait associations of ...  \n",
       "2   Give me the SNP-disease/trait associations of ...  \n",
       "3   Give me the SNP-disease/trait associations of ...  \n",
       "4   Give me the SNP-disease/trait associations of ...  \n",
       "..                                                ...  \n",
       "95  Give me the SNP-disease/trait associations of ...  \n",
       "96  Give me the SNP-disease/trait associations of ...  \n",
       "97  Give me the SNP-disease/trait associations of ...  \n",
       "98  Give me the SNP-disease/trait associations of ...  \n",
       "99  Give me the SNP-disease/trait associations of ...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=q*df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4170c8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Three of the four SNPs were significantly ass...</td>\n",
       "      <td>[ { \"genotype/allele\": \"-278 A/G\", \"rsID\": nul...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None of the variant alleles at any of these t...</td>\n",
       "      <td>[ { \"genotype/allele\": \"C\", \"rsID\": \"rs3781117...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Of the additional SNPs genotyped, rs4899445 d...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs489...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The variant genotypes at both SNP loci were s...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs125...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In SNP rs5498 E469(A/G), similar results were...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"AA\",\\n\"rsID\": \"rs549...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>However, the variant rs1052700*A was associat...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"A\",\\n\"rsID\": \"rs1052...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>The rs401681[C] allele has been positively as...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs4016...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>In the MSA-C subgroup the association strengt...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>We repeated the analysis in the MSA-C subgrou...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Two SNPs were significantly associated with i...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs5277...</td>\n",
       "      <td>Give me the SNP-disease/trait associations of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              context  \\\n",
       "0    Three of the four SNPs were significantly ass...   \n",
       "1    None of the variant alleles at any of these t...   \n",
       "2    Of the additional SNPs genotyped, rs4899445 d...   \n",
       "3    The variant genotypes at both SNP loci were s...   \n",
       "4    In SNP rs5498 E469(A/G), similar results were...   \n",
       "..                                                ...   \n",
       "95   However, the variant rs1052700*A was associat...   \n",
       "96   The rs401681[C] allele has been positively as...   \n",
       "97   In the MSA-C subgroup the association strengt...   \n",
       "98   We repeated the analysis in the MSA-C subgrou...   \n",
       "99   Two SNPs were significantly associated with i...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   [ { \"genotype/allele\": \"-278 A/G\", \"rsID\": nul...   \n",
       "1   [ { \"genotype/allele\": \"C\", \"rsID\": \"rs3781117...   \n",
       "2   [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs489...   \n",
       "3   [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs125...   \n",
       "4   [\\n{\\n\"genotype/allele\": \"AA\",\\n\"rsID\": \"rs549...   \n",
       "..                                                ...   \n",
       "95  [\\n{\\n\"genotype/allele\": \"A\",\\n\"rsID\": \"rs1052...   \n",
       "96  [\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs4016...   \n",
       "97  [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...   \n",
       "98  [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...   \n",
       "99  [\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs5277...   \n",
       "\n",
       "                                             question  \n",
       "0   Give me the SNP-disease/trait associations of ...  \n",
       "1   Give me the SNP-disease/trait associations of ...  \n",
       "2   Give me the SNP-disease/trait associations of ...  \n",
       "3   Give me the SNP-disease/trait associations of ...  \n",
       "4   Give me the SNP-disease/trait associations of ...  \n",
       "..                                                ...  \n",
       "95  Give me the SNP-disease/trait associations of ...  \n",
       "96  Give me the SNP-disease/trait associations of ...  \n",
       "97  Give me the SNP-disease/trait associations of ...  \n",
       "98  Give me the SNP-disease/trait associations of ...  \n",
       "99  Give me the SNP-disease/trait associations of ...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2611ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question']=q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4ed9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context = self.df.iloc[idx]['context']\n",
    "        question = self.df.iloc[idx]['question']\n",
    "        answer = self.df.iloc[idx]['answer']\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        token_type_ids = encoding['token_type_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        start_pos, end_pos = self.find_answer_indices(context, answer)\n",
    "        \n",
    "        return {'input_ids': input_ids, \n",
    "                'token_type_ids': token_type_ids, \n",
    "                'attention_mask': attention_mask,\n",
    "                'start_positions': start_pos, \n",
    "                'end_positions': end_pos}\n",
    "    \n",
    "    def find_answer_indices(self, context, answer):\n",
    "        start_pos = context.find(answer)\n",
    "        end_pos = start_pos + len(answer) - 1\n",
    "        return start_pos, end_pos\n",
    " \n",
    " \n",
    " \n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "dataset = QADataset(df, tokenizer)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d91fe0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22e301f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x185ab295ab0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe38af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0735badc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['context', 'answer', 'question'], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5040a98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ { \"genotype/allele\": \"-278 A/G\"'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].iloc[0].split(\",\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9058a85",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f1b4d304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT. Creating a new one with MEAN pooling.\n",
      "loading configuration file C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"C:\\\\Users\\\\AlexandruDaia/.cache\\\\torch\\\\sentence_transformers\\\\emilyalsentzer_Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"C:\\\\Users\\\\AlexandruDaia/.cache\\\\torch\\\\sentence_transformers\\\\emilyalsentzer_Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Didn't find file C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT\\tokenizer.json. We won't load it.\n",
      "Didn't find file C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT\\added_tokens.json. We won't load it.\n",
      "Didn't find file C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT\\special_tokens_map.json. We won't load it.\n",
      "Didn't find file C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT\\tokenizer_config.json. We won't load it.\n",
      "loading file C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT\\vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"C:\\\\Users\\\\AlexandruDaia/.cache\\\\torch\\\\sentence_transformers\\\\emilyalsentzer_Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file C:\\Users\\AlexandruDaia/.cache\\torch\\sentence_transformers\\emilyalsentzer_Bio_ClinicalBERT\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"C:\\\\Users\\\\AlexandruDaia/.cache\\\\torch\\\\sentence_transformers\\\\emilyalsentzer_Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable InputExample object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memilyalsentzer/Bio_ClinicalBERT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model on your dataset\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     20\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m SentenceEvaluator(examples)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:626\u001b[0m, in \u001b[0;36mSentenceTransformer.fit\u001b[1;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;66;03m##Add info to model card\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;66;03m#info_loss_functions = \"\\n\".join([\"- {} with {} training examples\".format(str(loss), len(dataloader)) for dataloader, loss in train_objectives])\u001b[39;00m\n\u001b[0;32m    625\u001b[0m info_loss_functions \u001b[38;5;241m=\u001b[39m  []\n\u001b[1;32m--> 626\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataloader, loss \u001b[38;5;129;01min\u001b[39;00m train_objectives:\n\u001b[0;32m    627\u001b[0m     info_loss_functions\u001b[38;5;241m.\u001b[39mextend(ModelCardTemplate\u001b[38;5;241m.\u001b[39mget_train_objective_info(dataloader, loss))\n\u001b[0;32m    628\u001b[0m info_loss_functions \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([text \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m info_loss_functions])\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable InputExample object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "\n",
    "# Load your dataset\n",
    "#df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Create a list of InputExamples from your dataset\n",
    "examples = []\n",
    "for index, row in df.iterrows():\n",
    "    examples.append(InputExample(texts=[row['text']], label=row['label'].split(\",\")))\n",
    "\n",
    "# Create the SentenceTransformer model\n",
    "model = SentenceTransformer( 'emilyalsentzer/Bio_ClinicalBERT')\n",
    "\n",
    "# Train the model on your dataset\n",
    "model.fit(examples)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = SentenceEvaluator(examples)\n",
    "evaluator.evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b778c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/config.json not found in cache or force_download set to True, downloading to C:\\Users\\AlexandruDaia\\.cache\\huggingface\\transformers\\tmpenbvp_61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023fe2176b7b47f48e1aae41ebeb49d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/config.json in cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\dc6d60ebe42d83e1479ce0d473758bb3586763ff6c4c814bda5321acf856bd64.b74d0770929e519c6d193d16b6874051ae549f5c8c228903a48e59d36260466b\n",
      "creating metadata file for C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\dc6d60ebe42d83e1479ce0d473758bb3586763ff6c4c814bda5321acf856bd64.b74d0770929e519c6d193d16b6874051ae549f5c8c228903a48e59d36260466b\n",
      "loading configuration file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/config.json from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\dc6d60ebe42d83e1479ce0d473758bb3586763ff6c4c814bda5321acf856bd64.b74d0770929e519c6d193d16b6874051ae549f5c8c228903a48e59d36260466b\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\AlexandruDaia\\.cache\\huggingface\\transformers\\tmpzrk5h4sw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bf36d53640424ea1dc684af9c3873b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/pytorch_model.bin in cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\794538e7c825dc7be96d9fc3c73b79a9736da5f699fc50d31513dbca0740b349.f0d8b668347b3048f5b88e273fde3c3412366726bc99aa5935b7990944092fb1\n",
      "creating metadata file for C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\794538e7c825dc7be96d9fc3c73b79a9736da5f699fc50d31513dbca0740b349.f0d8b668347b3048f5b88e273fde3c3412366726bc99aa5935b7990944092fb1\n",
      "loading weights file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/pytorch_model.bin from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\794538e7c825dc7be96d9fc3c73b79a9736da5f699fc50d31513dbca0740b349.f0d8b668347b3048f5b88e273fde3c3412366726bc99aa5935b7990944092fb1\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/config.json from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\dc6d60ebe42d83e1479ce0d473758bb3586763ff6c4c814bda5321acf856bd64.b74d0770929e519c6d193d16b6874051ae549f5c8c228903a48e59d36260466b\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to C:\\Users\\AlexandruDaia\\.cache\\huggingface\\transformers\\tmpg4wuko5s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6543cd2b974317b7dab0003730cce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/vocab.txt in cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\d544709b8432c5d7b9fef7d9361a9cc048632d93776ae61e401a6eff0fb8f037.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "creating metadata file for C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\d544709b8432c5d7b9fef7d9361a9cc048632d93776ae61e401a6eff0fb8f037.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/vocab.txt from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\d544709b8432c5d7b9fef7d9361a9cc048632d93776ae61e401a6eff0fb8f037.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/config.json from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\dc6d60ebe42d83e1479ce0d473758bb3586763ff6c4c814bda5321acf856bd64.b74d0770929e519c6d193d16b6874051ae549f5c8c228903a48e59d36260466b\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/config.json from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\dc6d60ebe42d83e1479ce0d473758bb3586763ff6c4c814bda5321acf856bd64.b74d0770929e519c6d193d16b6874051ae549f5c8c228903a48e59d36260466b\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertForQuestionAnswering' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memilyalsentzer/Bio_ClinicalBERT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(df, df)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertForQuestionAnswering' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "model.train()\n",
    "model.fit(df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2af3be68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "training mode is expected to be boolean",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1908\u001b[0m, in \u001b[0;36mModule.train\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sets the module in training mode.\u001b[39;00m\n\u001b[0;32m   1894\u001b[0m \n\u001b[0;32m   1895\u001b[0m \u001b[38;5;124;03mThis has any effect only on certain modules. See documentations of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1905\u001b[0m \u001b[38;5;124;03m    Module: self\u001b[39;00m\n\u001b[0;32m   1906\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mode, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m-> 1908\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining mode is expected to be boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1909\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m   1910\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n",
      "\u001b[1;31mValueError\u001b[0m: training mode is expected to be boolean"
     ]
    }
   ],
   "source": [
    "model.train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5195ab98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Three of the four SNPs were significantly ass...</td>\n",
       "      <td>[ { \"genotype/allele\": \"-278 A/G\", \"rsID\": nul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None of the variant alleles at any of these t...</td>\n",
       "      <td>[ { \"genotype/allele\": \"C\", \"rsID\": \"rs3781117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Of the additional SNPs genotyped, rs4899445 d...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs489...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The variant genotypes at both SNP loci were s...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In SNP rs5498 E469(A/G), similar results were...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"AA\",\\n\"rsID\": \"rs549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>However, the variant rs1052700*A was associat...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"A\",\\n\"rsID\": \"rs1052...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>The rs401681[C] allele has been positively as...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs4016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>In the MSA-C subgroup the association strengt...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>We repeated the analysis in the MSA-C subgrou...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Two SNPs were significantly associated with i...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs5277...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0    Three of the four SNPs were significantly ass...   \n",
       "1    None of the variant alleles at any of these t...   \n",
       "2    Of the additional SNPs genotyped, rs4899445 d...   \n",
       "3    The variant genotypes at both SNP loci were s...   \n",
       "4    In SNP rs5498 E469(A/G), similar results were...   \n",
       "..                                                ...   \n",
       "95   However, the variant rs1052700*A was associat...   \n",
       "96   The rs401681[C] allele has been positively as...   \n",
       "97   In the MSA-C subgroup the association strengt...   \n",
       "98   We repeated the analysis in the MSA-C subgrou...   \n",
       "99   Two SNPs were significantly associated with i...   \n",
       "\n",
       "                                                label  \n",
       "0   [ { \"genotype/allele\": \"-278 A/G\", \"rsID\": nul...  \n",
       "1   [ { \"genotype/allele\": \"C\", \"rsID\": \"rs3781117...  \n",
       "2   [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs489...  \n",
       "3   [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs125...  \n",
       "4   [\\n{\\n\"genotype/allele\": \"AA\",\\n\"rsID\": \"rs549...  \n",
       "..                                                ...  \n",
       "95  [\\n{\\n\"genotype/allele\": \"A\",\\n\"rsID\": \"rs1052...  \n",
       "96  [\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs4016...  \n",
       "97  [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...  \n",
       "98  [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...  \n",
       "99  [\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs5277...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1c95affd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/config.json from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\f048b8136bae2b3abe91e9e82949295fb205887c84db3be2775e1cdb0ecfeeb9.d7812d36d3371e4d43299a0c4a938622c5251db0efa17a5d4d9b57037fcec823\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\AlexandruDaia\\.cache\\huggingface\\transformers\\tmp5pzau7oh\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18e1c38b20f47ee9a909dd7877772d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/pytorch_model.bin in cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\65231a5792b14eb81b9a6bdccccfffda18575eb3bafbb730c9fa4235e56c3c17.74cc2087932cb523a583bd5e65732ee1aaade59dfc0b62f88101de7567d92e42\n",
      "creating metadata file for C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\65231a5792b14eb81b9a6bdccccfffda18575eb3bafbb730c9fa4235e56c3c17.74cc2087932cb523a583bd5e65732ee1aaade59dfc0b62f88101de7567d92e42\n",
      "loading weights file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/pytorch_model.bin from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\65231a5792b14eb81b9a6bdccccfffda18575eb3bafbb730c9fa4235e56c3c17.74cc2087932cb523a583bd5e65732ee1aaade59dfc0b62f88101de7567d92e42\n",
      "Loading PyTorch weights from C:\\Users\\AlexandruDaia\\.cache\\huggingface\\transformers\\65231a5792b14eb81b9a6bdccccfffda18575eb3bafbb730c9fa4235e56c3c17.74cc2087932cb523a583bd5e65732ee1aaade59dfc0b62f88101de7567d92e42\n",
      "PyTorch checkpoint contains 108,310,272 parameters\n",
      "Loaded 108,310,272 parameters in the TF 2.0 model.\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/vocab.txt from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\cda52d3a8283b321708097045e27f11cd70bbf3ad8cdefa2c0a56f187855f5d5.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/special_tokens_map.json from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\118da8438a7854000cfcf052566f83ae4f4159ac25796e49e16c3b18746041b4.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/tokenizer_config.json from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\2fb6c5805404829e9c10c33b38ae59ae3011225799f3177f769a06a7411fa46c.25d8d06fb0679146a3ed2a3463e3585380bff882fe6e1ebc497196e40dbbd7fa\n",
      "loading file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-v1.1/resolve/main/config.json from cache at C:\\Users\\AlexandruDaia/.cache\\huggingface\\transformers\\f048b8136bae2b3abe91e9e82949295fb205887c84db3be2775e1cdb0ecfeeb9.d7812d36d3371e4d43299a0c4a938622c5251db0efa17a5d4d9b57037fcec823\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-v1.1\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Temp\\ipykernel_10336\\2161090654.py\", line 43, in train_step  *\n        logits = model([input_ids, attention_masks])[0]\n    File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ALEXAN~1\\AppData\\Local\\Temp\\__autograph_generated_file_3dmposs.py\", line 17, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(inputs)['input_ids'], attention_mask=ag__.ld(inputs)['attention_mask'], token_type_ids=ag__.ld(inputs)['token_type_ids'], position_ids=ag__.ld(inputs)['position_ids'], head_mask=ag__.ld(inputs)['head_mask'], inputs_embeds=ag__.ld(inputs)['inputs_embeds'], output_attentions=ag__.ld(inputs)['output_attentions'], output_hidden_states=ag__.ld(inputs)['output_hidden_states'], return_dict=ag__.ld(inputs)['return_dict'], training=ag__.ld(inputs)['training']), fscope)\n    File \"C:\\Users\\ALEXAN~1\\AppData\\Local\\Temp\\__autograph_generated_file_2ffpvcv.py\", line 73, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"tf_bert_for_sequence_classification\" \"                 f\"(type TFBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1752, in call  *\n            outputs = self.bert(\n        File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\ALEXAN~1\\AppData\\Local\\Temp\\__autograph_generated_file_2ffpvcv.py\", line 73, in tf__call\n            (batch_size, seq_length) = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer \"bert\" \"                 f\"(type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 773, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: too many values to unpack (expected 2)\n        \n        \n        Call arguments received by layer \"bert\" \"                 f\"(type TFBertMainLayer):\n          • input_ids=tf.Tensor(shape=(32, 1, 512), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(32, 1, 512), dtype=int32)\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=False\n          • kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received by layer \"tf_bert_for_sequence_classification\" \"                 f\"(type TFBertForSequenceClassification):\n      • input_ids=['tf.Tensor(shape=(32, 1, 512), dtype=int32)', 'tf.Tensor(shape=(32, 1, 512), dtype=int32)']\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=False\n      • kwargs=<class 'inspect._empty'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[1;32m---> 53\u001b[0m         loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\ALEXAN~1\\AppData\\Local\\Temp\\__autograph_generated_fileayapq3yp.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(input_ids, attention_masks, labels)\u001b[0m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 11\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_masks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(loss), (ag__\u001b[38;5;241m.\u001b[39mld(labels), ag__\u001b[38;5;241m.\u001b[39mld(logits)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss_value), ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mtrainable_weights), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\ALEXAN~1\\AppData\\Local\\Temp\\__autograph_generated_file_3dmposs.py:17\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(input_processing), (), \u001b[38;5;28mdict\u001b[39m(func\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcall, config\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconfig, input_ids\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(input_ids), attention_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(attention_mask), token_type_ids\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(token_type_ids), position_ids\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(position_ids), head_mask\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(head_mask), inputs_embeds\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(inputs_embeds), output_attentions\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(output_attentions), output_hidden_states\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(output_hidden_states), return_dict\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(return_dict), labels\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(labels), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training), kwargs_call\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(kwargs)), fscope)\n\u001b[1;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoken_type_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposition_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhead_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minputs_embeds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_attentions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_hidden_states\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreturn_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(outputs)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdropout, (), \u001b[38;5;28mdict\u001b[39m(inputs\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(pooled_output), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]), fscope)\n",
      "File \u001b[1;32mC:\\Users\\ALEXAN~1\\AppData\\Local\\Temp\\__autograph_generated_file_2ffpvcv.py:73\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mand_(\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m (batch_size, seq_length) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(input_shape)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_4\u001b[39m():\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (ag__\u001b[38;5;241m.\u001b[39mldu(\u001b[38;5;28;01mlambda\u001b[39;00m : inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m), past_key_values_length)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Temp\\ipykernel_10336\\2161090654.py\", line 43, in train_step  *\n        logits = model([input_ids, attention_masks])[0]\n    File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ALEXAN~1\\AppData\\Local\\Temp\\__autograph_generated_file_3dmposs.py\", line 17, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(inputs)['input_ids'], attention_mask=ag__.ld(inputs)['attention_mask'], token_type_ids=ag__.ld(inputs)['token_type_ids'], position_ids=ag__.ld(inputs)['position_ids'], head_mask=ag__.ld(inputs)['head_mask'], inputs_embeds=ag__.ld(inputs)['inputs_embeds'], output_attentions=ag__.ld(inputs)['output_attentions'], output_hidden_states=ag__.ld(inputs)['output_hidden_states'], return_dict=ag__.ld(inputs)['return_dict'], training=ag__.ld(inputs)['training']), fscope)\n    File \"C:\\Users\\ALEXAN~1\\AppData\\Local\\Temp\\__autograph_generated_file_2ffpvcv.py\", line 73, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"tf_bert_for_sequence_classification\" \"                 f\"(type TFBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1752, in call  *\n            outputs = self.bert(\n        File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\ALEXAN~1\\AppData\\Local\\Temp\\__autograph_generated_file_2ffpvcv.py\", line 73, in tf__call\n            (batch_size, seq_length) = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer \"bert\" \"                 f\"(type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 773, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: too many values to unpack (expected 2)\n        \n        \n        Call arguments received by layer \"bert\" \"                 f\"(type TFBertMainLayer):\n          • input_ids=tf.Tensor(shape=(32, 1, 512), dtype=int32)\n          • attention_mask=tf.Tensor(shape=(32, 1, 512), dtype=int32)\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=False\n          • kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received by layer \"tf_bert_for_sequence_classification\" \"                 f\"(type TFBertForSequenceClassification):\n      • input_ids=['tf.Tensor(shape=(32, 1, 512), dtype=int32)', 'tf.Tensor(shape=(32, 1, 512), dtype=int32)']\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=False\n      • kwargs=<class 'inspect._empty'>\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the BioBERT pre-trained model and tokenizer\n",
    "model = TFBertForSequenceClassification.from_pretrained('dmis-lab/biobert-v1.1', num_labels=2, from_pt=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\n",
    "\n",
    "# Load the training data\n",
    "train_data = df\n",
    "\n",
    "# Tokenize the input text and convert it into numerical tokens\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for text in train_data['text']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                      # Text to encode\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,          # Max length of the text that can go to BERT\n",
    "                        pad_to_max_length = True,  # Pad the rest of the sequence to max length\n",
    "                        return_attention_mask = True, # Return attention mask\n",
    "                        return_tensors = 'tf',     # Return TensorFlow tensors\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the labels to numerical format\n",
    "labels = np.array(train_data['label'])\n",
    "\n",
    "# Create a TensorFlow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_masks, labels)).batch(32)\n",
    "\n",
    "# Define the optimizer, loss function, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "# Define the training loop\n",
    "@tf.function\n",
    "def train_step(input_ids, attention_masks, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model([input_ids, attention_masks])[0]\n",
    "        loss_value = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    metric.update_state(labels, logits)\n",
    "    return loss_value\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):\n",
    "    for step, batch in enumerate(dataset):\n",
    "        loss_value = train_step(batch[0], batch[1], batch[2])\n",
    "        if step % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Step {step}, Loss {loss_value}, Accuracy {metric.result()}')\n",
    "            metric.reset_states()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('bio_bert_summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ccbcbb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Three of the four SNPs were significantly ass...</td>\n",
       "      <td>[ { \"genotype/allele\": \"-278 A/G\", \"rsID\": nul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None of the variant alleles at any of these t...</td>\n",
       "      <td>[ { \"genotype/allele\": \"C\", \"rsID\": \"rs3781117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Of the additional SNPs genotyped, rs4899445 d...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs489...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The variant genotypes at both SNP loci were s...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In SNP rs5498 E469(A/G), similar results were...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"AA\",\\n\"rsID\": \"rs549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>However, the variant rs1052700*A was associat...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"A\",\\n\"rsID\": \"rs1052...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>The rs401681[C] allele has been positively as...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs4016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>In the MSA-C subgroup the association strengt...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>We repeated the analysis in the MSA-C subgrou...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Two SNPs were significantly associated with i...</td>\n",
       "      <td>[\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs5277...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0    Three of the four SNPs were significantly ass...   \n",
       "1    None of the variant alleles at any of these t...   \n",
       "2    Of the additional SNPs genotyped, rs4899445 d...   \n",
       "3    The variant genotypes at both SNP loci were s...   \n",
       "4    In SNP rs5498 E469(A/G), similar results were...   \n",
       "..                                                ...   \n",
       "95   However, the variant rs1052700*A was associat...   \n",
       "96   The rs401681[C] allele has been positively as...   \n",
       "97   In the MSA-C subgroup the association strengt...   \n",
       "98   We repeated the analysis in the MSA-C subgrou...   \n",
       "99   Two SNPs were significantly associated with i...   \n",
       "\n",
       "                                                label  \n",
       "0   [ { \"genotype/allele\": \"-278 A/G\", \"rsID\": nul...  \n",
       "1   [ { \"genotype/allele\": \"C\", \"rsID\": \"rs3781117...  \n",
       "2   [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs489...  \n",
       "3   [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs125...  \n",
       "4   [\\n{\\n\"genotype/allele\": \"AA\",\\n\"rsID\": \"rs549...  \n",
       "..                                                ...  \n",
       "95  [\\n{\\n\"genotype/allele\": \"A\",\\n\"rsID\": \"rs1052...  \n",
       "96  [\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs4016...  \n",
       "97  [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...  \n",
       "98  [\\n{\\n\"genotype/allele\": null,\\n\"rsID\": \"rs382...  \n",
       "99  [\\n{\\n\"genotype/allele\": \"C\",\\n\"rsID\": \"rs5277...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f20d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
