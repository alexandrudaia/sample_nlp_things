{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7c2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key =\"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\"\n",
    "\n",
    "start_sequence = \"\\nA:\"\n",
    "restart_sequence = \"\\n\\nQ: \"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"Give me the SNP-disease/trait associations of the following paragraph in csv format (rsID, disease, allele, genotype, odds ratio, p-value, ethnicity, sex,...) : Poly(2-methyl- and 2-ethylthioadenylic acid) were prepared by polymerization of corresponding diphosphates with Escherichia coli polynucleotide phosphorylase. These polynucleotides have relatively large hypochromicity of 30-35%. Acid titration of these polymers showed abrupt transition at pH 5.34-5.4, which may indicate that the introduction of alkylthio group at 2-position of adenine bases reduced their basicity. Thermal melting of these polymers showed no clear transition points at neutral pH, but in acidic media they have Tm values of 57 and 56 degrees C, somewhat lower than that of poly(A). Upon complex formation with poly(U), these poly(A) analogs showed only one poly(rs2A) . poly(U) type double-strand complexes, similar to that found in the case of poly(m2A) . poly(U )\\n \",\n",
    "  temperature=0.9,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5823bf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrs2A, Poly(2-methyl- and 2-ethylthioadenylic acid) polymerization, Alkylthio group introduction at 2-position of adenine bases, -, -, -, -, -'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a2ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('RelevantParas_baseline_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766943af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Paragraph'].iloc[60][10:][:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb5930f",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "    current_row=train['Paragraph'].iloc[65][10:][:-10]\n",
    "    prpt=\"Please give me the SNP-disease/trait associations of the following paragraph : \" +str(current_row)\n",
    "    #print(\"question \" , prpt)\n",
    "    start_sequence = \"\\nA:\"\n",
    "    restart_sequence = \"\\n\\nQ: \"\n",
    "    openai.api_key =\"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\"\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prpt,\n",
    "        temperature=0.3,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=[\"\\n\"]\n",
    "    )\n",
    "    print(response[\"choices\"][0][\"text\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3443e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641a999",
   "metadata": {},
   "outputs": [],
   "source": [
    " row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed6562",
   "metadata": {},
   "outputs": [],
   "source": [
    " train['Paragraph'][row][10:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet  transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet pytorch-lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99053e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705dea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6048660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd09296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e954ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b310096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1595eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb919fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data  import Dataset ,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44afc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d59760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks  import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52976dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e02fa16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5badaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8138332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ( \n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration ,\n",
    "    T5TokenizerFast as T5Tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f040fb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1373544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gdown --id  1mxVUywvKzvA9bvrUc11RYuOTy7MYcXHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c992766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gdown\n",
    "#!pip3 install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q  bio-QA.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a3bcdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path('bio-QA\\BioASQ\\BioASQ-train-factoid-4b.json').open() as  json_file:\n",
    "    data=json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4e11f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'version'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82e6bfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BioASQ6b'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25717ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=data[\"data\"][0][\"paragraphs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeff4661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qas': [{'id': '52bf208003868f1b06000019_002',\n",
       "   'question': 'What is the inheritance pattern of Liâ€“Fraumeni syndrome?',\n",
       "   'answers': [{'text': 'autosomal dominant', 'answer_start': 213}]}],\n",
       " 'context': 'Balanced t(11;15)(q23;q15) in a TP53+/+ breast cancer patient from a Li-Fraumeni syndrome family. Li-Fraumeni Syndrome (LFS) is characterized by early-onset carcinogenesis involving multiple tumor types and shows autosomal dominant inheritance. Approximately 70% of LFS cases are due to germline mutations in the TP53 gene on chromosome 17p13.1. Mutations have also been found in the CHEK2 gene on chromosome 22q11, and others have been mapped to chromosome 11q23. While characterizing an LFS family with a documented defect in TP53, we found one family member who developed bilateral breast cancer at age 37 yet was homozygous for wild-type TP53. Her mother also developed early-onset primary bilateral breast cancer, and a sister had unilateral breast cancer and a soft tissue sarcoma. Cytogenetic analysis using fluorescence in situ hybridization of a primary skin fibroblast cell line revealed that the patient had a novel balanced reciprocal translocation between the long arms of chromosomes 11 and 15: t(11;15)(q23;q15). This translocation was not present in a primary skin fibroblast cell line from a brother with neuroblastoma, who was heterozygous for the TP53 mutation. There was no evidence of acute lymphoblastic leukemia in either the patient or her mother, although a nephew did develop leukemia and died in childhood. These data may implicate the region at breakpoint 11q23 and/or 15q15 as playing a significant role in predisposition to breast cancer development.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdb01515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_and_answers(factoid_path : Path):\n",
    "    with factoid_path.open() as json_file:\n",
    "        data=json.load(json_file)\n",
    "    questions=data[\"data\"][0][\"paragraphs\"]\n",
    "    data_rows=[]\n",
    "    for question in questions:\n",
    "        context=question[\"context\"]\n",
    "        for question_and_answers  in question['qas']:\n",
    "            question=question_and_answers['question']\n",
    "            answers=question_and_answers['answers']\n",
    "            for  answer in answers:\n",
    "                answer_text=answer['text']\n",
    "                answer_start=answer['answer_start']\n",
    "                answer_end=answer_start+len(answer_text)\n",
    "                data_rows.append({\n",
    "                    \"question\":question,\n",
    "                    \"context\":context,\n",
    "                    \"answer_text\":answer_text,\n",
    "                    \"answer_start\":answer_start,\n",
    "                    \"answer_end\":answer_end\n",
    "                })\n",
    "    return pd.DataFrame(data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e092cf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the inheritance pattern of Liâ€“Fraumeni...</td>\n",
       "      <td>Balanced t(11;15)(q23;q15) in a TP53+/+ breast...</td>\n",
       "      <td>autosomal dominant</td>\n",
       "      <td>213</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the inheritance pattern of Liâ€“Fraumeni...</td>\n",
       "      <td>Genetic modeling of Li-Fraumeni syndrome in ze...</td>\n",
       "      <td>autosomal dominant</td>\n",
       "      <td>105</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which type of lung cancer is afatinib used for?</td>\n",
       "      <td>Clinical perspective of afatinib in non-small ...</td>\n",
       "      <td>EGFR-mutant NSCLC</td>\n",
       "      <td>1203</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which hormone abnormalities are characteristic...</td>\n",
       "      <td>DOCA sensitive pendrin expression in kidney, h...</td>\n",
       "      <td>thyroid</td>\n",
       "      <td>419</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which hormone abnormalities are characteristic...</td>\n",
       "      <td>Clinical and molecular characteristics of Pend...</td>\n",
       "      <td>thyroid</td>\n",
       "      <td>705</td>\n",
       "      <td>712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the inheritance pattern of Liâ€“Fraumeni...   \n",
       "1  What is the inheritance pattern of Liâ€“Fraumeni...   \n",
       "2    Which type of lung cancer is afatinib used for?   \n",
       "3  Which hormone abnormalities are characteristic...   \n",
       "4  Which hormone abnormalities are characteristic...   \n",
       "\n",
       "                                             context         answer_text  \\\n",
       "0  Balanced t(11;15)(q23;q15) in a TP53+/+ breast...  autosomal dominant   \n",
       "1  Genetic modeling of Li-Fraumeni syndrome in ze...  autosomal dominant   \n",
       "2  Clinical perspective of afatinib in non-small ...   EGFR-mutant NSCLC   \n",
       "3  DOCA sensitive pendrin expression in kidney, h...             thyroid   \n",
       "4  Clinical and molecular characteristics of Pend...             thyroid   \n",
       "\n",
       "   answer_start  answer_end  \n",
       "0           213         231  \n",
       "1           105         123  \n",
       "2          1203        1220  \n",
       "3           419         426  \n",
       "4           705         712  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_questions_and_answers(Path('bio-QA\\BioASQ\\BioASQ-train-factoid-4b.json')).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "265a905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "factoid_paths=sorted(list(Path(\"bio-QA/BioASQ/\").glob(\"BioASQ-train-*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32cc850b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('bio-QA/BioASQ/BioASQ-train-factoid-4b.json'),\n",
       " WindowsPath('bio-QA/BioASQ/BioASQ-train-factoid-5b.json'),\n",
       " WindowsPath('bio-QA/BioASQ/BioASQ-train-factoid-6b.json')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factoid_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb28788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=[]\n",
    "for factoid_path in factoid_paths:\n",
    "    dfs.append(extract_questions_and_answers(factoid_path))\n",
    "df=pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddfd8f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the inheritance pattern of Liâ€“Fraumeni...</td>\n",
       "      <td>Balanced t(11;15)(q23;q15) in a TP53+/+ breast...</td>\n",
       "      <td>autosomal dominant</td>\n",
       "      <td>213</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the inheritance pattern of Liâ€“Fraumeni...</td>\n",
       "      <td>Genetic modeling of Li-Fraumeni syndrome in ze...</td>\n",
       "      <td>autosomal dominant</td>\n",
       "      <td>105</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which type of lung cancer is afatinib used for?</td>\n",
       "      <td>Clinical perspective of afatinib in non-small ...</td>\n",
       "      <td>EGFR-mutant NSCLC</td>\n",
       "      <td>1203</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which hormone abnormalities are characteristic...</td>\n",
       "      <td>DOCA sensitive pendrin expression in kidney, h...</td>\n",
       "      <td>thyroid</td>\n",
       "      <td>419</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which hormone abnormalities are characteristic...</td>\n",
       "      <td>Clinical and molecular characteristics of Pend...</td>\n",
       "      <td>thyroid</td>\n",
       "      <td>705</td>\n",
       "      <td>712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the inheritance pattern of Liâ€“Fraumeni...   \n",
       "1  What is the inheritance pattern of Liâ€“Fraumeni...   \n",
       "2    Which type of lung cancer is afatinib used for?   \n",
       "3  Which hormone abnormalities are characteristic...   \n",
       "4  Which hormone abnormalities are characteristic...   \n",
       "\n",
       "                                             context         answer_text  \\\n",
       "0  Balanced t(11;15)(q23;q15) in a TP53+/+ breast...  autosomal dominant   \n",
       "1  Genetic modeling of Li-Fraumeni syndrome in ze...  autosomal dominant   \n",
       "2  Clinical perspective of afatinib in non-small ...   EGFR-mutant NSCLC   \n",
       "3  DOCA sensitive pendrin expression in kidney, h...             thyroid   \n",
       "4  Clinical and molecular characteristics of Pend...             thyroid   \n",
       "\n",
       "   answer_start  answer_end  \n",
       "0           213         231  \n",
       "1           105         123  \n",
       "2          1203        1220  \n",
       "3           419         426  \n",
       "4           705         712  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee639b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12988, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31b32212",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates(subset=[\"context\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43d1c237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2582, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94000f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.question.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa1d89ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question        What is the characteristic feature of the Dyke...\n",
       "context         Left hemisphere and male sex dominance of cere...\n",
       "answer_text                                  cerebral hemiatrophy\n",
       "answer_start                                                  130\n",
       "answer_end                                                    150\n",
       "Name: 240, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_question=df.iloc[240]\n",
    "sample_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26379cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_answer(question):\n",
    "    answer_start , answer_end=question['answer_start'],question['answer_end']\n",
    "    context=question['context']\n",
    "    return colored(context[:answer_start],'white')+\\\n",
    "      colored(context[answer_start:answer_end+1],'green')+\\\n",
    "      colored(context[answer_end+1:],\"white\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85f0d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the characteristic feature of the Dyke-Davidoff-Masson syndrome.\n",
      "\n",
      "Answer:\n",
      "\u001b[37mLeft hemisphere and male sex dominance of cerebral hemiatrophy (Dyke-Davidoff-Masson Syndrome).\n",
      "Although radiological findings of \u001b[0m\u001b[32mcerebral hemiatrophy \u001b[0m\u001b[37m(Dyke-Davidoff-Masson\n",
      "Syndrome) are well known, there is no systematic study about the gender and the affected side in\n",
      "this syndrome. Brain images in 26 patients (mean aged 11) with cerebral hemiatrophy were\n",
      "retrospectively reviewed. Nineteen patients (73.5%) were male and seven patients (26.5%) were\n",
      "female. Left hemisphere involvement was seen in 18 patients (69.2%) and right hemisphere involvement\n",
      "was seen in eight patients (30.8%). We conclude that male gender and left side involvement are\n",
      "frequent in cerebral hemiatrophy disease.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(sample_question[\"question\"])\n",
    "print()\n",
    "print(\"Answer:\")\n",
    "for wrap in textwrap.wrap(color_answer(sample_question),width=100):\n",
    "    print(wrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28f90587",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07cfb854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer=T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "617b8b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoding=tokenizer(\n",
    "    \"Would I rather  be feared or loved?\",\n",
    "    \"Easy.Both.I want people to be afraid of how much they love me .\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d4d2cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c18648f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5328, 27, 1066, 36, 3, 27625, 42, 1858, 58, 1, 6844, 5, 26465, 107, 5, 196, 241, 151, 12, 36, 7403, 13, 149, 231, 79, 333, 140, 3, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "print(sample_encoding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65811e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(sample_encoding[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "516383ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=[\n",
    "    tokenizer.decode(input_id,skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    for input_id  in sample_encoding[\"input_ids\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e634be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Would I rather be  feared or loved ?  Easy . Bot h . I want people to be afraid of how much they love me  . '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \" \".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71f17568",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding=tokenizer(\n",
    "    sample_question[\"question\"],\n",
    "    sample_question[\"context\"],\n",
    "    max_length=396,\n",
    "    padding=\"max_length\",\n",
    "    truncation=\"only_second\",\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45f5ab26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BatchEncoding.keys of {'input_ids': tensor([[  363,    19,     8, 16115,  1451,    13,     8, 12991,  1050,    18,\n",
       "           308,     9,  6961,  1647,    18, 27189,   106, 12398,     5,     1,\n",
       "         14298,     3,  6015,    23,  9475,    11,  5069,     3,     7,   994,\n",
       "         10138,   663,    13, 24387,     3,   107, 11658,    17, 29006,    41,\n",
       "           308,    63,  1050,    18,   308,     9,  6961,  1647,    18, 27189,\n",
       "           106, 27956,   137,  1875,  2252,  6207,  7469,    13, 24387,     3,\n",
       "           107, 11658,    17, 29006,    41,   308,    63,  1050,    18,   308,\n",
       "             9,  6961,  1647,    18, 27189,   106, 27956,    61,    33,   168,\n",
       "           801,     6,   132,    19,   150, 20036,   810,    81,     8,  7285,\n",
       "            11,     8,  4161,   596,    16,    48, 12398,     5, 14170,  1383,\n",
       "            16,  2208,  1221,    41,   526,   152,  9742,   850,    61,    28,\n",
       "         24387,     3,   107, 11658,    17, 29006,   130, 29825,   120,  9112,\n",
       "             5, 19636,  6808,  1221, 13649,  9285,  6210,   130,  5069,    11,\n",
       "          2391,  1221,  4743, 17255,  6210,   130,  3955,     5, 14298,     3,\n",
       "          6015,    23,  9475,  9683,    47,   894,    16,   507,  1221,    41,\n",
       "          3951,     5,  5406,    61,    11,   269,     3,  6015,    23,  9475,\n",
       "          9683,    47,   894,    16,  2641,  1221, 19684,     5,  5953,   137,\n",
       "           101, 12692,    24,  5069,  7285,    11,   646,   596,  9683,    33,\n",
       "          8325,    16, 24387,     3,   107, 11658,    17, 29006,  1994,     5,\n",
       "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04321b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>',\n",
       " 'additional_special_tokens': ['<extra_id_0>',\n",
       "  '<extra_id_1>',\n",
       "  '<extra_id_2>',\n",
       "  '<extra_id_3>',\n",
       "  '<extra_id_4>',\n",
       "  '<extra_id_5>',\n",
       "  '<extra_id_6>',\n",
       "  '<extra_id_7>',\n",
       "  '<extra_id_8>',\n",
       "  '<extra_id_9>',\n",
       "  '<extra_id_10>',\n",
       "  '<extra_id_11>',\n",
       "  '<extra_id_12>',\n",
       "  '<extra_id_13>',\n",
       "  '<extra_id_14>',\n",
       "  '<extra_id_15>',\n",
       "  '<extra_id_16>',\n",
       "  '<extra_id_17>',\n",
       "  '<extra_id_18>',\n",
       "  '<extra_id_19>',\n",
       "  '<extra_id_20>',\n",
       "  '<extra_id_21>',\n",
       "  '<extra_id_22>',\n",
       "  '<extra_id_23>',\n",
       "  '<extra_id_24>',\n",
       "  '<extra_id_25>',\n",
       "  '<extra_id_26>',\n",
       "  '<extra_id_27>',\n",
       "  '<extra_id_28>',\n",
       "  '<extra_id_29>',\n",
       "  '<extra_id_30>',\n",
       "  '<extra_id_31>',\n",
       "  '<extra_id_32>',\n",
       "  '<extra_id_33>',\n",
       "  '<extra_id_34>',\n",
       "  '<extra_id_35>',\n",
       "  '<extra_id_36>',\n",
       "  '<extra_id_37>',\n",
       "  '<extra_id_38>',\n",
       "  '<extra_id_39>',\n",
       "  '<extra_id_40>',\n",
       "  '<extra_id_41>',\n",
       "  '<extra_id_42>',\n",
       "  '<extra_id_43>',\n",
       "  '<extra_id_44>',\n",
       "  '<extra_id_45>',\n",
       "  '<extra_id_46>',\n",
       "  '<extra_id_47>',\n",
       "  '<extra_id_48>',\n",
       "  '<extra_id_49>',\n",
       "  '<extra_id_50>',\n",
       "  '<extra_id_51>',\n",
       "  '<extra_id_52>',\n",
       "  '<extra_id_53>',\n",
       "  '<extra_id_54>',\n",
       "  '<extra_id_55>',\n",
       "  '<extra_id_56>',\n",
       "  '<extra_id_57>',\n",
       "  '<extra_id_58>',\n",
       "  '<extra_id_59>',\n",
       "  '<extra_id_60>',\n",
       "  '<extra_id_61>',\n",
       "  '<extra_id_62>',\n",
       "  '<extra_id_63>',\n",
       "  '<extra_id_64>',\n",
       "  '<extra_id_65>',\n",
       "  '<extra_id_66>',\n",
       "  '<extra_id_67>',\n",
       "  '<extra_id_68>',\n",
       "  '<extra_id_69>',\n",
       "  '<extra_id_70>',\n",
       "  '<extra_id_71>',\n",
       "  '<extra_id_72>',\n",
       "  '<extra_id_73>',\n",
       "  '<extra_id_74>',\n",
       "  '<extra_id_75>',\n",
       "  '<extra_id_76>',\n",
       "  '<extra_id_77>',\n",
       "  '<extra_id_78>',\n",
       "  '<extra_id_79>',\n",
       "  '<extra_id_80>',\n",
       "  '<extra_id_81>',\n",
       "  '<extra_id_82>',\n",
       "  '<extra_id_83>',\n",
       "  '<extra_id_84>',\n",
       "  '<extra_id_85>',\n",
       "  '<extra_id_86>',\n",
       "  '<extra_id_87>',\n",
       "  '<extra_id_88>',\n",
       "  '<extra_id_89>',\n",
       "  '<extra_id_90>',\n",
       "  '<extra_id_91>',\n",
       "  '<extra_id_92>',\n",
       "  '<extra_id_93>',\n",
       "  '<extra_id_94>',\n",
       "  '<extra_id_95>',\n",
       "  '<extra_id_96>',\n",
       "  '<extra_id_97>',\n",
       "  '<extra_id_98>',\n",
       "  '<extra_id_99>']}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57fc59dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token , tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69da7f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the characteristic feature of the Dyke-Davidoff-Masson syndrome.</s> Left hemisphere and male sex dominance of cerebral hemiatrophy (Dyke-Davidoff-Masson Syndrome). Although radiological findings of cerebral hemiatrophy (Dyke-Davidoff-Masson Syndrome) are well known, there is no systematic study about the gender and the affected side in this syndrome. Brain images in 26 patients (mean aged 11) with cerebral hemiatrophy were retrospectively reviewed. Nineteen patients (73.5%) were male and seven patients (26.5%) were female. Left hemisphere involvement was seen in 18 patients (69.2%) and right hemisphere involvement was seen in eight patients (30.8%). We conclude that male gender and left side involvement are frequent in cerebral hemiatrophy disease.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding[\"input_ids\"].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25f0af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_encoding=tokenizer(\n",
    "    sample_question[\"answer_text\"],\n",
    "  \n",
    "    max_length=32,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab5e7915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cerebral hemiatrophy</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(answer_encoding['input_ids'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "006c525a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24387,     3,   107, 11658,    17, 29006,     1,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=answer_encoding[\"input_ids\"]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ae3ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[labels == 0]=-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0f20a80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BioQADataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sample_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mBioQADataset\u001b[49m(df,tokenizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BioQADataset' is not defined"
     ]
    }
   ],
   "source": [
    "sample_dataset=BioQADataset(df,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8091de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,val_df=train_test_split(df, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32050d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioQADataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        source_max_token_len: int = 396,\n",
    "        target_max_token_len: int = 32\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_row = self.data.iloc[index]\n",
    "        \n",
    "        source_encoding = self.tokenizer(\n",
    "            data_row['question'],\n",
    "            data_row['context'],\n",
    "            max_length = self.source_max_token_len,\n",
    "            padding = 'max_length',\n",
    "            truncation = 'only_second',\n",
    "            return_attention_mask = True,\n",
    "            add_special_tokens = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        try:\n",
    "            target_encoding = self.tokenizer(\n",
    "                data_row['answer_text'],\n",
    "                max_length = self.target_max_token_len,\n",
    "                padding = 'max_length',\n",
    "                truncation = True,\n",
    "                return_attention_mask = True,\n",
    "                add_special_tokens = True,\n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "        except:\n",
    "            print(data_row['answer_text'])\n",
    "        \n",
    "        labels = target_encoding['input_ids']\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return dict(\n",
    "            question = data_row['question'],\n",
    "            context = data_row['context'],\n",
    "            answer_text = data_row['answer_text'],\n",
    "            input_ids = source_encoding['input_ids'].flatten(),\n",
    "            attention_mask = source_encoding['attention_mask'].flatten(),\n",
    "            labels = labels.flatten()\n",
    "        )\n",
    "    \n",
    "\n",
    "class BioQADataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        batch_size: int = 8,\n",
    "        source_max_token_len: int = 396,\n",
    "        target_max_token_len: int = 32        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "        \n",
    "        self.train_dataset = BioQADataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len\n",
    "        )\n",
    "\n",
    "        self.val_dataset = BioQADataset(\n",
    "            self.val_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len\n",
    "        )\n",
    "        \n",
    "        self.test_dataset = BioQADataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = True,\n",
    "            num_workers = 4,\n",
    "            drop_last=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size = 1,\n",
    "            num_workers = 4,\n",
    "            drop_last=True\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size = 1,\n",
    "            num_workers = 4,\n",
    "            drop_last=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "827cea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6cf90caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71e1bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module=BioQADataModule(train_df,val_df,val_df,tokenizer,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c7bdecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecd8db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=T5ForConditionalGeneration.from_pretrained(MODEL_NAME,return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "35fe5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=tokenizer(\n",
    "    \"translate English to Russian:I talk a lot , so I've learned to tune myself out\",\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33f48b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AlexandruDaia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  1674,  1131,    15,  2221,     3,     6,    92,  2010,     3,\n",
       "           362, 29484,     6,  2278,  2619,   170,     3,    17,  1864,    76]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids=model.generate(input_ids=input_ids)\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e0d27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=[\n",
    "    tokenizer.decode(gen_id,skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    for gen_id in generated_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3811baf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich rede viel, also habe ich gelernt, mich selbst zu tÃ¤u']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be853e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich rede viel, also habe ich gelernt, mich selbst zu tÃ¤u'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e10417a6",
   "metadata": {},
   "outputs": [],
   "source": [
    " class BioQAModel(pl.LightningModule):\n",
    "   def __init__(self):\n",
    "     super().__init__()\n",
    "     self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "   def forward(self, input_ids, attention_mask, labels=None):\n",
    "     output = self.model(\n",
    "         input_ids, \n",
    "         attention_mask=attention_mask,\n",
    "         labels=labels)\n",
    "     return output.loss, output.logits\n",
    "   def training_step(self, batch, batch_idx):\n",
    "     input_ids = batch['input_ids']\n",
    "     attention_mask=batch['attention_mask']\n",
    "     labels = batch['labels']\n",
    "     loss, outputs = self(input_ids, attention_mask, labels)\n",
    "     self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "     return {\"loss\": loss, \"predictions\":outputs, \"labels\": labels}\n",
    "   def validation_step(self, batch, batch_idx):\n",
    "     input_ids = batch['input_ids']\n",
    "     attention_mask=batch['attention_mask']\n",
    "     labels = batch['labels']\n",
    "     loss, outputs = self(input_ids, attention_mask, labels)\n",
    "     self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "     return loss\n",
    "   def test_step(self, batch, batch_idx):\n",
    "     input_ids = batch['input_ids']\n",
    "     attention_mask=batch['attention_mask']\n",
    "     labels = batch['labels']\n",
    "     loss, outputs = self(input_ids, attention_mask, labels)\n",
    "     self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "     return loss\n",
    "   def configure_optimizers(self):\n",
    "     optimizer = AdamW(self.parameters(), lr=0.0001)\n",
    "     return optimizer\n",
    " model = BioQAModel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa7e092b",
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "No supported gpu backend found!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m      2\u001b[0m    dirpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints_1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m    filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest-checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m    mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m   \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m   \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m   \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m   \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m64\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m training_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py:348\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:419\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, inference_mode)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m DataConnector(\u001b[38;5;28mself\u001b[39m, multiple_trainloader_mode)\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43mAcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_cores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mipus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mipus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_select_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_select_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:203\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[1;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_auto_accelerator()\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_choose_gpu_accelerator_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_parallel_devices_and_init_accelerator()\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:538\u001b[0m, in \u001b[0;36mAcceleratorConnector._choose_gpu_accelerator_backend\u001b[1;34m()\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CUDAAccelerator\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 538\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo supported gpu backend found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: No supported gpu backend found!"
     ]
    }
   ],
   "source": [
    " checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints_1\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback,  ],\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=N_EPOCHS,\n",
    "    devices=1,\n",
    "    precision='64'\n",
    ")\n",
    "\n",
    "print(\"Starting training\")\n",
    "training_start = time.time()\n",
    "trainer.fit(model, data_module)\n",
    "training_time = time.time()-training_start\n",
    "print(\"Training done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22722998",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c79600",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d8759fb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TQDMProgressBar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m----> 2\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback, \u001b[43mTQDMProgressBar\u001b[49m(refresh_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)],\n\u001b[0;32m      3\u001b[0m     gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      4\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mN_EPOCHS,\n\u001b[0;32m      5\u001b[0m  \n\u001b[0;32m      6\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#devices=1,\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     precision\u001b[38;5;241m=\u001b[39mfp_precision\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TQDMProgressBar' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback, TQDMProgressBar(refresh_rate=30)],\n",
    "    gpus=1,\n",
    "    max_epochs=N_EPOCHS,\n",
    " \n",
    "    accelerator='cpu', \n",
    "    #devices=1,\n",
    "    precision=fp_precision\n",
    ")\n",
    "import time\n",
    "print(\"Starting training\")\n",
    "training_start = time.time()\n",
    "trainer.fit(model, data_module)\n",
    "training_time = time.time()-training_start\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "123c6351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "da85cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7640fcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '2'\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7349422b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb8ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
